{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a07919-8bb6-41b3-962c-90000192a0bf",
   "metadata": {},
   "source": [
    "#### Learning what this work is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce3427dd-9113-431f-b3d9-b8c248b43fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from torch.cuda.amp import autocast\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4810f5c6-3610-47ad-8ae5-d41d0d71ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "if (not torch.cuda.is_available()):\n",
    "    print(\"GPU not availabe\")\n",
    "\n",
    "MODEL_NAME = \"../llama3/Meta-Llama-3-8B/\"\n",
    "WEIGHTS_PATH = \"../llama3/Meta-Llama-3-8B/path-to-finetuned-model\"\n",
    "MAX_LENGTH=256\n",
    "BATCH_SIZE=4\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42812fe3-c310-4aae-b537-276c25bbea5a",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10df821f-fb61-430b-9353-1cf1521d5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../lmsys-chatbot-arena/test.csv')\n",
    "sample_sub = pd.read_csv('../lmsys-chatbot-arena/sample_submission.csv')\n",
    "\n",
    "# reformat the input texts\n",
    "def process(input_str):\n",
    "    # remove the [ and ] and the begin and end of text\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    # split the string with \",\", then remove \"\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    # combine the strings with space\n",
    "    return ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2be62f01-6d4a-407b-9039-78ea12f67294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  winner_model_a  winner_model_b  winner_tie\n",
       "0   136060        0.333333        0.333333    0.333333\n",
       "1   211333        0.333333        0.333333    0.333333\n",
       "2  1233961        0.333333        0.333333    0.333333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>I have three oranges today, I ate an orange ye...</td>\n",
       "      <td>You have two oranges today.</td>\n",
       "      <td>You still have three oranges. Eating an orange...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>You are a mediator in a heated political debat...</td>\n",
       "      <td>Thank you for sharing the details of the situa...</td>\n",
       "      <td>Mr Reddy and Ms Blue both have valid points in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>How to initialize the classification head when...</td>\n",
       "      <td>When you want to initialize the classification...</td>\n",
       "      <td>To initialize the classification head when per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             prompt  \\\n",
       "0   136060  I have three oranges today, I ate an orange ye...   \n",
       "1   211333  You are a mediator in a heated political debat...   \n",
       "2  1233961  How to initialize the classification head when...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0                        You have two oranges today.   \n",
       "1  Thank you for sharing the details of the situa...   \n",
       "2  When you want to initialize the classification...   \n",
       "\n",
       "                                          response_b  \n",
       "0  You still have three oranges. Eating an orange...  \n",
       "1  Mr Reddy and Ms Blue both have valid points in...  \n",
       "2  To initialize the classification head when per...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User prompt: I have three oranges today, I ate an orange yesterday. How many oranges do I have?\n",
      "\n",
      "Model A :\n",
      "You have two oranges today.\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "You still have three oranges. Eating an orange yesterday does not affect the number of oranges you have today.\n"
     ]
    }
   ],
   "source": [
    "test.loc[:,'prompt']=test['prompt'].apply(process)\n",
    "test.loc[:,'response_a']=test['response_a'].apply(process)\n",
    "test.loc[:,'response_b']=test['response_b'].apply(process)\n",
    "display(sample_sub)\n",
    "display(test.head(5))\n",
    "\n",
    "test['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + test['response_b']\n",
    "print(test['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e2143-2093-4884-be04-717fe786fdab",
   "metadata": {},
   "source": [
    "### tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdb04a1d-866b-4867-b8e4-76403732bad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INPUT_IDS</th>\n",
       "      <th>ATTENTION_MASK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128000, 1502, 10137, 25, 358, 617, 2380, 8513...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[128000, 1502, 10137, 25, 1472, 527, 264, 6903...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           INPUT_IDS  \\\n",
       "0  [128000, 1502, 10137, 25, 358, 617, 2380, 8513...   \n",
       "1  [128000, 1502, 10137, 25, 1472, 527, 264, 6903...   \n",
       "\n",
       "                                      ATTENTION_MASK  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../llama3/Meta-Llama-3-8B/\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_size = 'right'\n",
    "tokenizer.add_eos_token = True\n",
    "tokens = tokenizer(test['text'].tolist(), padding='max_length', max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n",
    "INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)\n",
    "ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n",
    "\n",
    "input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\n",
    "attention_mask_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['INPUT_IDS'] = input_ids_cpu\n",
    "data['ATTENTION_MASK'] = attention_mask_cpu\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fda48f-1d6a-4b85-baa1-f288c81bbd80",
   "metadata": {},
   "source": [
    "## Load Model & weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea26fd4f-5ec1-46a3-a3e7-5261d5759064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a75f2ee4cf4962b277c7652e22170f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ../llama3/Meta-Llama-3-8B/ and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit = True,\n",
    "    bnb_8bit_compute_dtype = torch.float16,\n",
    "    bnb_8bit_use_double_quant=False)\n",
    "\n",
    "# load model to GPU\n",
    "device_single = torch.device('cuda')\n",
    "base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels = 3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda')\n",
    "\n",
    "base_model.config.pad_token_id=tokenizer.pad_token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57f3dad-fc7b-4f0a-9501-7aea29beea59",
   "metadata": {},
   "source": [
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5971ab-6b26-4783-a56c-70572e6a4b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lora configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=1,\n",
    "    lora_alpha = 2,\n",
    "    lora_dropout = 0.1,\n",
    "    bias = 'none',\n",
    "    inference_mode = True,\n",
    "    task_type = TaskType.SEQ_CLS,\n",
    "    target_modules = ['o_proj','v_proj']\n",
    ")\n",
    "\n",
    "# get peft\n",
    "model = get_peft_model(base_model, peft_config).to(device_single)\n",
    "model.load_state_dict(torch.load(WEIGHTS_PATH), strict = False)\n",
    "model.eval()\n",
    "\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2c7457-3e98-4b46-b91a-15f4034afc0d",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed29288b-a0b0-4174-8a15-29573809f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# define the inference function (the original design has two gpu models, which requires a function to handle inference)\n",
    "def inference(df, model, device, batch_size = BATCH_SIZE):\n",
    "    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n",
    "    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n",
    "    generated_class_a = []\n",
    "    generated_class_b = []\n",
    "    generated_class_c = []\n",
    "\n",
    "    model.eval()\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        # consider when the training reaches the end of training\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        # get the batched input id and attention mask\n",
    "        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n",
    "        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n",
    "        # get the inference output\n",
    "        with torch.no_grad():\n",
    "            with autocast():\n",
    "                ouputs = model(\n",
    "                    input_ids = batch_input_ids,\n",
    "                    attention_mask = batch_attention_mask\n",
    "                )\n",
    "        # apply softmax to get the probabilities\n",
    "        probabilities = torch.softmax(outputs.logits, dim = -1).cpu().numpy()\n",
    "        # append probabilities to the results\n",
    "        generated_class_a.extend(probabilities[:, 0])\n",
    "        generated_class_b.extend(probabilities[:, 0])\n",
    "        generated_class_c.extend(probabilities[:, 0])\n",
    "\n",
    "    df['winner_model_a']=generated_class_a\n",
    "    df['winner_model_b']=generated_class_b\n",
    "    df['winner_tie']=generated_class_c\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return df\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "N_SAMPLE = len(data)\n",
    "\n",
    "# inference\n",
    "# May need to use a larger gpu to test the inference function with finetuned model. \n",
    "# For now, continued to the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c569cff-12b2-4953-8a21-4e37e2c10091",
   "metadata": {},
   "source": [
    "## LGBM + tfdif\n",
    "learn how the tfdif is used in this inference. Can test train with this feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21ef99-e180-4f40-be73-666b0c199485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
