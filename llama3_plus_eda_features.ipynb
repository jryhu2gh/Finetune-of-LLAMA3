{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "007d454f-b97b-4e51-ac4d-617b45cf56a5",
   "metadata": {},
   "source": [
    "### llama3_plus_tfidf features\n",
    "explore how the tfidf features can help with the classification problem\n",
    "balabala"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c185f-c653-46e4-9d6f-e21f6c79b56e",
   "metadata": {},
   "source": [
    "### generate tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b006ab39-d740-47ea-832d-96114004c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import related packages\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, Dict, List, Optional\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a93d5a-a552-4090-b5c7-8405de1fb74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../lmsys-chatbot-arena/'\n",
    "TARGETS = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc1bfe3e-02d5-43d9-ac9d-24410f143b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_csv(DATA_PATH+'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f71c28fc-3d6b-465d-9fa5-c437c9cdad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _concat_turns(\n",
    "    strs: List[Optional[str]],\n",
    "    sep: str = ' ',\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Concat Multiple turns of prompts/responses into a single string\n",
    "    Args:\n",
    "        strs: A list of strings.\n",
    "        sep: the seperate character or word.\n",
    "\n",
    "    Returns:\n",
    "    The concatenated string\n",
    "    \"\"\"\n",
    "    # get rid of None str\n",
    "    stripped_strs=strs.strip('[]')\n",
    "    sentence = [s.strip('\"') for s in stripped_strs.split('\",\"')]    \n",
    "    cat_str = sep.join(sentence)\n",
    "    return cat_str\n",
    "\n",
    "def _cal_tf(word: str, doc: str) -> float:\n",
    "    \"\"\" Calculate and return the term frequency.\"\"\"\n",
    "    tokens = doc.split()\n",
    "    # term count\n",
    "    tc = sum(1 for token in tokens if token == word)\n",
    "    tf = tc/len(tokens)\n",
    "\n",
    "    return tf\n",
    "\n",
    "def _cal_idf(word: str, corpus: List[str]) -> float:\n",
    "    \"\"\" Calculate and return the inverse document frequency.\"\"\"\n",
    "    n_w = 0\n",
    "    for doc in corpus:\n",
    "        if word in doc.split():\n",
    "            n_w += 1\n",
    "\n",
    "    idf = np.log(len(corpus)/(n_w+1))\n",
    "\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c37821-e5ce-458c-bedd-94372003c0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "      <th>cat_prompt</th>\n",
       "      <th>cat_res_a</th>\n",
       "      <th>cat_res_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Is it morally right to try to have a certain p...</td>\n",
       "      <td>The question of whether it is morally right to...</td>\n",
       "      <td>As an AI, I don't have personal beliefs or opi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id             model_a     model_b  \\\n",
       "0  30192  gpt-4-1106-preview  gpt-4-0613   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "\n",
       "   winner_model_b  winner_tie  \\\n",
       "0               0           0   \n",
       "\n",
       "                                          cat_prompt  \\\n",
       "0  Is it morally right to try to have a certain p...   \n",
       "\n",
       "                                           cat_res_a  \\\n",
       "0  The question of whether it is morally right to...   \n",
       "\n",
       "                                           cat_res_b  \n",
       "0  As an AI, I don't have personal beliefs or opi...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"cat_prompt\"] = train[\"prompt\"].apply(lambda x: _concat_turns(x))\n",
    "train[\"cat_res_a\"] = train[\"response_a\"].apply(lambda x: _concat_turns(x))\n",
    "train[\"cat_res_b\"] = train[\"response_b\"].apply(lambda x: _concat_turns(x))\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e380343f-674f-490e-8fef-d7e604c9d961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Word \"to\" <<<\n",
      "## Doc 0 ##\n",
      "TF 0.0690 | IDF 0.4308 | TF-IDF 0.0297\n",
      "--------------------------------------------------\n",
      ">>>Word \"I\" <<<\n",
      "## Doc 3 ##\n",
      "TF 0.1111 | IDF 1.6094 | TF-IDF 0.1788\n",
      "--------------------------------------------------\n",
      ">>>Word \"possibility\" <<<\n",
      "## Doc 19 ##\n",
      "TF 0.0175 | IDF 2.3026 | TF-IDF 0.0404\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "demo_corpus = train[\"cat_prompt\"].iloc[:20].tolist()\n",
    "for demo_word, doc_id in zip([\"to\", \"I\", \"possibility\"], [0, 3, 19]):\n",
    "    print(f\">>>Word \\\"{demo_word}\\\" <<<\")\n",
    "    print(f\"## Doc {doc_id} ##\")\n",
    "\n",
    "    tf = _cal_tf(demo_word, demo_corpus[doc_id])\n",
    "    idf = _cal_idf(demo_word, demo_corpus)\n",
    "    tfidf = tf*idf\n",
    "\n",
    "    print(f\"TF {tf:.4f} | IDF {idf:.4f} | TF-IDF {tfidf:.4f}\")\n",
    "    print(f\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f683f5ab-6ecd-4698-8ddf-fc2ed3e1ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_cfg = {\n",
    "    \"input\": \"content\",\n",
    "    \"lowercase\": True,\n",
    "    \"analyzer\": \"word\",\n",
    "    \"ngram_range\": (1,3),\n",
    "    \"max_df\": 0.95,\n",
    "    \"min_df\": 10,\n",
    "    \"max_features\": 300,\n",
    "    \"smooth_idf\": True,\n",
    "    \"sublinear_tf\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "211ee78b-4d54-453d-a7d1-3a7342d81943",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =  [\"cat_prompt\", \"cat_res_a\", \"cat_res_b\"]\n",
    "vectorizers = {col:TfidfVectorizer(**vectorizer_cfg) for col in cols}\n",
    "\n",
    "#generate tf-idf features\n",
    "# X_tfidf = []\n",
    "# for col, vectorizer in vectorizers.item():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c34d8135-1485-440a-862a-552dcba03567",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = []\n",
    "for col, vectorizer in vectorizers.items():\n",
    "    x=vectorizer.fit_transform(train[col])\n",
    "    X_tfidf.append(x.toarray())\n",
    "X_tfidf = np.hstack(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8578f3c-1567-43a9-83f1-6caccf430632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inverse Documnet Frequency Vector ===\n",
      ">>> cat_prompt <<<\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t0</th>\n",
       "      <th>u2022</th>\n",
       "      <th>comment</th>\n",
       "      <th>int</th>\n",
       "      <th>id</th>\n",
       "      <th>enable</th>\n",
       "      <th>image</th>\n",
       "      <th>self</th>\n",
       "      <th>the user</th>\n",
       "      <th>class</th>\n",
       "      <th>...</th>\n",
       "      <th>that</th>\n",
       "      <th>you</th>\n",
       "      <th>for</th>\n",
       "      <th>what</th>\n",
       "      <th>and</th>\n",
       "      <th>is</th>\n",
       "      <th>in</th>\n",
       "      <th>of</th>\n",
       "      <th>to</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.251107</td>\n",
       "      <td>6.902912</td>\n",
       "      <td>6.331536</td>\n",
       "      <td>6.169197</td>\n",
       "      <td>6.145027</td>\n",
       "      <td>5.952804</td>\n",
       "      <td>5.688169</td>\n",
       "      <td>5.631221</td>\n",
       "      <td>5.625878</td>\n",
       "      <td>5.47758</td>\n",
       "      <td>...</td>\n",
       "      <td>2.60195</td>\n",
       "      <td>2.548574</td>\n",
       "      <td>2.407855</td>\n",
       "      <td>2.312888</td>\n",
       "      <td>2.01309</td>\n",
       "      <td>1.999952</td>\n",
       "      <td>1.985212</td>\n",
       "      <td>1.963364</td>\n",
       "      <td>1.915473</td>\n",
       "      <td>1.614227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         t0     u2022   comment       int        id    enable     image  \\\n",
       "0  9.251107  6.902912  6.331536  6.169197  6.145027  5.952804  5.688169   \n",
       "\n",
       "       self  the user    class  ...     that       you       for      what  \\\n",
       "0  5.631221  5.625878  5.47758  ...  2.60195  2.548574  2.407855  2.312888   \n",
       "\n",
       "       and        is        in        of        to       the  \n",
       "0  2.01309  1.999952  1.985212  1.963364  1.915473  1.614227  \n",
       "\n",
       "[1 rows x 300 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> cat_res_a <<<\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u0438</th>\n",
       "      <th>u0442</th>\n",
       "      <th>u043e</th>\n",
       "      <th>u0430</th>\n",
       "      <th>u0435</th>\n",
       "      <th>u2022</th>\n",
       "      <th>de</th>\n",
       "      <th>string</th>\n",
       "      <th>company</th>\n",
       "      <th>energy</th>\n",
       "      <th>...</th>\n",
       "      <th>with</th>\n",
       "      <th>it</th>\n",
       "      <th>for</th>\n",
       "      <th>that</th>\n",
       "      <th>is</th>\n",
       "      <th>in</th>\n",
       "      <th>of</th>\n",
       "      <th>and</th>\n",
       "      <th>to</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.935277</td>\n",
       "      <td>6.915732</td>\n",
       "      <td>6.896563</td>\n",
       "      <td>6.890253</td>\n",
       "      <td>6.859291</td>\n",
       "      <td>5.565567</td>\n",
       "      <td>5.317975</td>\n",
       "      <td>4.75278</td>\n",
       "      <td>4.551233</td>\n",
       "      <td>4.402207</td>\n",
       "      <td>...</td>\n",
       "      <td>1.607401</td>\n",
       "      <td>1.604117</td>\n",
       "      <td>1.502849</td>\n",
       "      <td>1.480321</td>\n",
       "      <td>1.363749</td>\n",
       "      <td>1.334737</td>\n",
       "      <td>1.258118</td>\n",
       "      <td>1.224785</td>\n",
       "      <td>1.212251</td>\n",
       "      <td>1.152728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      u0438     u0442     u043e     u0430     u0435     u2022        de  \\\n",
       "0  6.935277  6.915732  6.896563  6.890253  6.859291  5.565567  5.317975   \n",
       "\n",
       "    string   company    energy  ...      with        it       for      that  \\\n",
       "0  4.75278  4.551233  4.402207  ...  1.607401  1.604117  1.502849  1.480321   \n",
       "\n",
       "         is        in        of       and        to       the  \n",
       "0  1.363749  1.334737  1.258118  1.224785  1.212251  1.152728  \n",
       "\n",
       "[1 rows x 300 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> cat_res_b <<<\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u0438</th>\n",
       "      <th>u0442</th>\n",
       "      <th>u043d</th>\n",
       "      <th>u0435</th>\n",
       "      <th>u0430</th>\n",
       "      <th>u043e</th>\n",
       "      <th>u2022</th>\n",
       "      <th>de</th>\n",
       "      <th>string</th>\n",
       "      <th>company</th>\n",
       "      <th>...</th>\n",
       "      <th>with</th>\n",
       "      <th>it</th>\n",
       "      <th>for</th>\n",
       "      <th>that</th>\n",
       "      <th>is</th>\n",
       "      <th>in</th>\n",
       "      <th>of</th>\n",
       "      <th>and</th>\n",
       "      <th>to</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.915732</td>\n",
       "      <td>6.877753</td>\n",
       "      <td>6.859291</td>\n",
       "      <td>6.841164</td>\n",
       "      <td>6.841164</td>\n",
       "      <td>6.823359</td>\n",
       "      <td>5.555583</td>\n",
       "      <td>5.328474</td>\n",
       "      <td>4.762471</td>\n",
       "      <td>4.526082</td>\n",
       "      <td>...</td>\n",
       "      <td>1.604658</td>\n",
       "      <td>1.595717</td>\n",
       "      <td>1.505787</td>\n",
       "      <td>1.479</td>\n",
       "      <td>1.361398</td>\n",
       "      <td>1.3352</td>\n",
       "      <td>1.254409</td>\n",
       "      <td>1.225744</td>\n",
       "      <td>1.208837</td>\n",
       "      <td>1.154899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      u0438     u0442     u043d     u0435     u0430     u043e     u2022  \\\n",
       "0  6.915732  6.877753  6.859291  6.841164  6.841164  6.823359  5.555583   \n",
       "\n",
       "         de    string   company  ...      with        it       for   that  \\\n",
       "0  5.328474  4.762471  4.526082  ...  1.604658  1.595717  1.505787  1.479   \n",
       "\n",
       "         is      in        of       and        to       the  \n",
       "0  1.361398  1.3352  1.254409  1.225744  1.208837  1.154899  \n",
       "\n",
       "[1 rows x 300 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"=== Inverse Documnet Frequency Vector ===\")\n",
    "for col, vec in vectorizers.items():\n",
    "    print(f\">>> {col} <<<\")\n",
    "    tmp = pd.DataFrame(vec.idf_, index=vec.get_feature_names_out()).sort_values(0, ascending=False)\n",
    "    display(tmp.T)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f166375-f3f9-4a7e-a8aa-5d0bd800b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_feats = vectorizers[\"cat_prompt\"].get_feature_names_out()\n",
    "res_a_feats = vectorizers[\"cat_res_a\"].get_feature_names_out()\n",
    "res_b_feats = vectorizers[\"cat_res_b\"].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "babec9d4-e2c7-4a91-85f7-554f6e087aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "tfidf_feats = (\n",
    "    [f\"{feat}(prompt)\" for feat in prompt_feats]\n",
    "    + [f\"{feat}(res_a)\" for feat in res_a_feats]\n",
    "    + [f\"{feat}(res_b)\" for feat in res_b_feats]\n",
    ")\n",
    "X = pd.DataFrame(X_tfidf, columns = tfidf_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83a095d2-a5f6-44ee-9f0f-7b77cd66be9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10(prompt)</th>\n",
       "      <th>100(prompt)</th>\n",
       "      <th>20(prompt)</th>\n",
       "      <th>about(prompt)</th>\n",
       "      <th>about the(prompt)</th>\n",
       "      <th>act(prompt)</th>\n",
       "      <th>action(prompt)</th>\n",
       "      <th>add(prompt)</th>\n",
       "      <th>after(prompt)</th>\n",
       "      <th>ai(prompt)</th>\n",
       "      <th>...</th>\n",
       "      <th>world(res_b)</th>\n",
       "      <th>would(res_b)</th>\n",
       "      <th>would be(res_b)</th>\n",
       "      <th>years(res_b)</th>\n",
       "      <th>you(res_b)</th>\n",
       "      <th>you are(res_b)</th>\n",
       "      <th>you can(res_b)</th>\n",
       "      <th>you have(res_b)</th>\n",
       "      <th>you re(res_b)</th>\n",
       "      <th>your(res_b)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.201281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.118866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068789</td>\n",
       "      <td>0.074145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 900 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10(prompt)  100(prompt)  20(prompt)  about(prompt)  about the(prompt)  \\\n",
       "0    0.000000          0.0         0.0            0.0                0.0   \n",
       "1    0.201281          0.0         0.0            0.0                0.0   \n",
       "\n",
       "   act(prompt)  action(prompt)  add(prompt)  after(prompt)  ai(prompt)  ...  \\\n",
       "0          0.0             0.0          0.0            0.0         0.0  ...   \n",
       "1          0.0             0.0          0.0            0.0         0.0  ...   \n",
       "\n",
       "   world(res_b)  would(res_b)  would be(res_b)  years(res_b)  you(res_b)  \\\n",
       "0      0.107871           0.0              0.0           0.0    0.061752   \n",
       "1      0.000000           0.0              0.0           0.0    0.118866   \n",
       "\n",
       "   you are(res_b)  you can(res_b)  you have(res_b)  you re(res_b)  your(res_b)  \n",
       "0             0.0        0.000000         0.000000            0.0     0.079803  \n",
       "1             0.0        0.068789         0.074145            0.0     0.131302  \n",
       "\n",
       "[2 rows x 900 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3cf0eb6-5b82-4478-963d-cee7e4c63212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "idf values:\n",
      "aaa : 1.916290731874155\n",
      "for : 1.916290731874155\n",
      "geeks : 1.2231435513142097\n",
      "r2j : 1.916290731874155\n"
     ]
    }
   ],
   "source": [
    "# Example. Tested that idf is calculated as np.log((nt+1)/(nd+1))+1\n",
    "# assign documents\n",
    "d0 = 'Geeks for geeks'\n",
    "d1 = 'Geeks'\n",
    "d2 = 'r2j Geeks'\n",
    "d3 = 'aaa '\n",
    "\n",
    "# merge documents into a single corpus\n",
    "string = [d0, d1, d2, d3]\n",
    "\n",
    "# create object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# get tf-df values\n",
    "result = tfidf.fit_transform(string)\n",
    "\n",
    "# get idf values\n",
    "print('\\nidf values:')\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):\n",
    "\tprint(ele1, ':', ele2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd0356b-ddfb-4c59-97b4-3d62cf211ddb",
   "metadata": {},
   "source": [
    "# Create inference model based on llama3\n",
    "In the training network, we hope to combine the llama3 with the above tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b24c7493-15ca-4eab-bc1a-5e9d2d62e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# finetuning related modules\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "# end\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "946644da-f37d-4f52-a9a1-85eccd8561a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is used\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    NUM_EPOCHS = 1\n",
    "    BATCH_SIZE = 2\n",
    "    DROPOUT = 0.05\n",
    "    MODEL_NAME = \"../llama3/Meta-Llama-3-8B/\"\n",
    "    SEED = 2024\n",
    "    MAX_LENGTH = 128 # truncate the input to save memory, toy implementation only\n",
    "    NUM_WARMUP_STEPS = 4 # toy implementation. \n",
    "    LR_MAX = 5E-5\n",
    "    NUM_CLASS_LLAMA = 128 # with the llama model, we hope it can generate 128 features, which are combined with tfidf features.\n",
    "    NUM_LABELS = 3 # The final number of labels\n",
    "    LORA_RANK = 1 # Toy implementation \n",
    "    LORA_ALPHA = 2 # toy implementation\n",
    "    LORA_MODULES = ['o_proj', 'v_proj']\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    print('GPU is used')\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    print('CPU is used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed9dc090-f5b0-4831-83bc-07f3f0e4a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b25fbb2-d097-46e7-8ab1-c6226983b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to ensure reproducibility\n",
    "def set_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seeds(seed=CFG.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8d86f0-4999-4db3-8c01-75d1c1de07ac",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57c02e4f-c99b-46f0-8429-3235a45a64a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tokenizer\\\\tokenizer_config.json',\n",
       " 'tokenizer\\\\special_tokens_map.json',\n",
       " 'tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_size = 'right'\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.tokenize(\"shuo bu shuo!\")\n",
    "\n",
    "tokenizer.save_pretrained('tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "240ad283-4fe8-4b52-84b8-e2797b5fed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function giving token length\n",
    "# only takes data frame input?\n",
    "def get_token_lengths(texts):\n",
    "    # tokenize and receive inputs_ids for each text\n",
    "    inputs_ids = tokenizer(texts.tolist(),return_tensors='np')['input_ids']\n",
    "    # input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']\n",
    "    # return length of inputs_ids for each text\n",
    "    return [len(t) for t in inputs_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6675200a-8589-46fa-a079-469537dc745b",
   "metadata": {},
   "source": [
    "### Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbab1c7d-146e-41dc-b19f-ba4f9b7f919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 19 Null reponse rows dropped\n",
      "Total train samples:  57458\n"
     ]
    }
   ],
   "source": [
    "# Drop 'Null' for training\n",
    "condition = (train.cat_res_a=='null') & (train.cat_res_b=='null')\n",
    "indexes = train[condition].index\n",
    "# print(indexes)\n",
    "train.drop(indexes, inplace=True)\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(f\"Total {len(indexes)} Null reponse rows dropped\")\n",
    "print('Total train samples: ', len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4850ffab-b9d8-4cd1-9f2f-a86687f8e505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User prompt: What is the best way to travel from Tel-Aviv to Jerusalem? Car? Bus? Plane?\n",
      "\n",
      "Model A:\n",
      "The best way to travel from Tel Aviv to Jerusalem depends on your personal preference and the availability of transportation options. All the options you have mentioned are valid options, but here are some details to help you make your decision:\\n\\n*   By car: Traveling by car is the quickest way to get from Tel Aviv to Jerusalem, as the distance between the two cities is only about 60 kilometers (37 miles). It takes around 45 minutes to drive from Tel Aviv to Jerusalem by car, depending on the traffic.\\n*   By bus: There are several bus lines that run from Tel Aviv to Jerusalem, and the journey takes around 1 hour and 30 minutes by bus. The buses are comfortable and reliable, and they offer a scenic view of the beautiful Israeli countryside.\\n*   By plane: There are no direct flights from Tel Aviv to Jerusalem, so you need to take a flight from Tel Aviv's Ben Gurion International Airport to Jerusalem's Sde Dov Airport. The flight takes around 30 minutes, and it is a convenient option if you are short on time.\\n\\nIn general, if you have enough time and don't mind taking the bus, I recommend that option. The buses are comfortable, reliable, and offer a scenic view of the countryside. If you are short on time and don't want to take a bus, a flight is another option to consider.\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "The best way to travel from Tel-Aviv to Jerusalem depends on your personal preferences and circumstances. \\n\\nIf you have a car and enjoy driving, then driving from Tel-Aviv to Jerusalem is an option. However, parking in Jerusalem can be challenging and expensive.\\n\\nIf you prefer to use public transportation, there are several bus lines that operate between Tel-Aviv and Jerusalem. Some of the most popular bus companies include Egged and Dan. The bus ride typically takes about an hour, depending on traffic.\\n\\nTaking a plane is not a recommended option since Tel-Aviv and Jerusalem are relatively close cities, and there are no airports in Jerusalem. \\n\\nIn summary, taking a bus is the most commonly used and convenient way to travel from Tel-Aviv to Jerusalem.\n"
     ]
    }
   ],
   "source": [
    "# Show the conversation\n",
    "train['text']='User prompt: '+train['cat_prompt']+ '\\n\\nModel A:\\n'+train['cat_res_a']+'\\n\\n--------\\n\\nModel B:\\n'+train['cat_res_b']\n",
    "print(train['text'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe073349-3c0f-421f-82d3-06ecd60bacf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "      <th>cat_prompt</th>\n",
       "      <th>cat_res_a</th>\n",
       "      <th>cat_res_b</th>\n",
       "      <th>text</th>\n",
       "      <th>token_count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Is it morally right to try to have a certain p...</td>\n",
       "      <td>The question of whether it is morally right to...</td>\n",
       "      <td>As an AI, I don't have personal beliefs or opi...</td>\n",
       "      <td>User prompt: Is it morally right to try to hav...</td>\n",
       "      <td>1206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>What is the difference between marriage licens...</td>\n",
       "      <td>A marriage license is a legal document that al...</td>\n",
       "      <td>A marriage license and a marriage certificate ...</td>\n",
       "      <td>User prompt: What is the difference between ma...</td>\n",
       "      <td>1393</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>[\"explain function calling. how would you call...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>explain function calling. how would you call a...</td>\n",
       "      <td>Function calling is the process of invoking or...</td>\n",
       "      <td>Function calling is the process of invoking a ...</td>\n",
       "      <td>User prompt: explain function calling. how wou...</td>\n",
       "      <td>664</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96401</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>[\"How can I create a test set for a very rare ...</td>\n",
       "      <td>[\"Creating a test set for a very rare category...</td>\n",
       "      <td>[\"When building a classifier for a very rare c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>How can I create a test set for a very rare ca...</td>\n",
       "      <td>Creating a test set for a very rare category c...</td>\n",
       "      <td>When building a classifier for a very rare cat...</td>\n",
       "      <td>User prompt: How can I create a test set for a...</td>\n",
       "      <td>1008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198779</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n",
       "      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n",
       "      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>What is the best way to travel from Tel-Aviv t...</td>\n",
       "      <td>The best way to travel from Tel Aviv to Jerusa...</td>\n",
       "      <td>The best way to travel from Tel-Aviv to Jerusa...</td>\n",
       "      <td>User prompt: What is the best way to travel fr...</td>\n",
       "      <td>479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             model_a              model_b  \\\n",
       "0   30192  gpt-4-1106-preview           gpt-4-0613   \n",
       "1   53567           koala-13b           gpt-4-0613   \n",
       "2   65089  gpt-3.5-turbo-0613       mistral-medium   \n",
       "3   96401    llama-2-13b-chat  mistral-7b-instruct   \n",
       "4  198779           koala-13b   gpt-3.5-turbo-0314   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "1  [\"What is the difference between marriage lice...   \n",
       "2  [\"explain function calling. how would you call...   \n",
       "3  [\"How can I create a test set for a very rare ...   \n",
       "4  [\"What is the best way to travel from Tel-Aviv...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "1  [\"A marriage license is a legal document that ...   \n",
       "2  [\"Function calling is the process of invoking ...   \n",
       "3  [\"Creating a test set for a very rare category...   \n",
       "4  [\"The best way to travel from Tel Aviv to Jeru...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1  [\"A marriage license and a marriage certificat...               0   \n",
       "2  [\"Function calling is the process of invoking ...               0   \n",
       "3  [\"When building a classifier for a very rare c...               1   \n",
       "4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \\\n",
       "0               0           0   \n",
       "1               1           0   \n",
       "2               0           1   \n",
       "3               0           0   \n",
       "4               1           0   \n",
       "\n",
       "                                          cat_prompt  \\\n",
       "0  Is it morally right to try to have a certain p...   \n",
       "1  What is the difference between marriage licens...   \n",
       "2  explain function calling. how would you call a...   \n",
       "3  How can I create a test set for a very rare ca...   \n",
       "4  What is the best way to travel from Tel-Aviv t...   \n",
       "\n",
       "                                           cat_res_a  \\\n",
       "0  The question of whether it is morally right to...   \n",
       "1  A marriage license is a legal document that al...   \n",
       "2  Function calling is the process of invoking or...   \n",
       "3  Creating a test set for a very rare category c...   \n",
       "4  The best way to travel from Tel Aviv to Jerusa...   \n",
       "\n",
       "                                           cat_res_b  \\\n",
       "0  As an AI, I don't have personal beliefs or opi...   \n",
       "1  A marriage license and a marriage certificate ...   \n",
       "2  Function calling is the process of invoking a ...   \n",
       "3  When building a classifier for a very rare cat...   \n",
       "4  The best way to travel from Tel-Aviv to Jerusa...   \n",
       "\n",
       "                                                text  token_count  label  \n",
       "0  User prompt: Is it morally right to try to hav...         1206      0  \n",
       "1  User prompt: What is the difference between ma...         1393      1  \n",
       "2  User prompt: explain function calling. how wou...          664      2  \n",
       "3  User prompt: How can I create a test set for a...         1008      0  \n",
       "4  User prompt: What is the best way to travel fr...          479      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train only 10% train dataset for toy implementation\n",
    "train = train[:int(len(train)/10)]\n",
    "texts = train['text']\n",
    "train.loc[:,'token_count'] = get_token_lengths(texts)\n",
    "\n",
    "# prepare label for model\n",
    "train.loc[:, 'label']=np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d241dd7c-d3fa-4a88-a24c-b79e22ca64c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    2018\n",
       "1    1968\n",
       "2    1759\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a8c1d01-4acd-4dec-9041-c51fd53255ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5745.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>730.840557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>821.874756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>282.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>553.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>892.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15428.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        token_count\n",
       "count   5745.000000\n",
       "mean     730.840557\n",
       "std      821.874756\n",
       "min       18.000000\n",
       "25%      282.000000\n",
       "50%      553.000000\n",
       "75%      892.000000\n",
       "max    15428.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# token Count\n",
    "display(train['token_count'].describe().to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2152dc84-d175-4e28-bad4-c3676816ace7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1392.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get length of tokens which covers 90% of data, we'll still take 128 length in this example for toy implementation\n",
    "np.percentile(train['token_count'],90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dbc78a-e1aa-451e-9a17-4c44bca38b4f",
   "metadata": {},
   "source": [
    "#### Tokenize train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aeffdf92-f849-45b1-b717-10b901479f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I', 'Ġhave', 'Ġ', '0', '.', '636', '193', '74', 'Ġapple', '<|end_of_text|>']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(CFG.MAX_LENGTH)\n",
    "a = tokenizer.tokenize(\"I have 0.63619374 apple\",padding='max_length',max_length=10,truncation=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dda8dec2-a8e5-4164-b9fe-22d6ae156d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_IDS shape: (5745, 128), ATTENTION_MASKS shape: (5745, 128)\n",
      "LABELS shape: (5745, 3)\n"
     ]
    }
   ],
   "source": [
    "# tokenize data\n",
    "tokens = tokenizer(\n",
    "    train['text'].tolist(),\n",
    "    padding='max_length',\n",
    "    max_length=CFG.MAX_LENGTH,\n",
    "    truncation=True,\n",
    "    return_tensors='np'\n",
    ")\n",
    "\n",
    "# Input IDs are teh token IDs\n",
    "INPUT_IDS = tokens['input_ids']\n",
    "# Attention Masks to Ignore Padding Tokens\n",
    "ATTENTION_MASKS = tokens['attention_mask']\n",
    "# Label of Texts\n",
    "LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values\n",
    "\n",
    "print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')\n",
    "print(f'LABELS shape: {LABELS.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c48f6b74-76d0-4124-8334-49040c7bd109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a543766d-e25d-42ed-a658-9671b271db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset function, create batch and return input_ids, attention_mask, X_feat(features), and labels\n",
    "def train_dataset(batch_size):\n",
    "    N_SAMPLES = LABELS.shape[0]\n",
    "    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))\n",
    "    while True:\n",
    "        # Shuffle Indices\n",
    "        np.random.shuffle(IDXS)\n",
    "        # Iterate Over All Indices Once\n",
    "        for idxs in IDXS.reshape(-1, batch_size):\n",
    "            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)\n",
    "            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)\n",
    "            labels = torch.tensor(LABELS[idxs]).to(DEVICE) # Multi-label\n",
    "            X_feat = torch.tensor(X_tfidf[idxs]).to(DEVICE)\n",
    "            # yield returns a returns a generator object to \n",
    "            # the one who calls the function which contains \n",
    "            # yield, instead of simply returning a value\n",
    "            yield input_ids, attention_mask, X_feat, labels\n",
    "            \n",
    "TRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca3ff642-5d33-458d-b37c-204a056d3bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128]) torch.Size([2, 128]) torch.Size([2, 900]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "## test the above function\n",
    "test_a, test_b, test_c, test_d = next(TRAIN_DATASET)\n",
    "print(test_a.shape, test_b.shape, test_c.shape, test_d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94e1ff-a072-41ef-ab5f-d04ecd8c02e1",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be5122ae-cb43-4c9a-b7fe-1f904ea355eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c766ccd2c6476eb4d82da70a3b7533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ../llama3/Meta-Llama-3-8B/ and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model for classification with 3 target label\n",
    "base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "    CFG.MODEL_NAME,\n",
    "    num_labels=CFG.NUM_CLASS_LLAMA,\n",
    "    torch_dtype = torch.bfloat16)\n",
    "\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "# Assign Padding TOKEN\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fddbfc-a9df-4b4f-8e66-3c253baee494",
   "metadata": {},
   "source": [
    "### Low-Rank Adaption [LORA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96a3a654-5b88-4493-ae3d-128f58b08a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.LORA_RANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "687efb69-7347-4e4b-9f84-1da7da865b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=CFG.LORA_RANK, # the dimension of the low-rank matrices\n",
    "    lora_alpha = CFG.LORA_ALPHA, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "    lora_dropout = CFG.DROPOUT,\n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type = TaskType.SEQ_CLS, # refer to https://github.com/huggingface/peft/blob/v0.8.2/src/peft/utils/peft_types.py#L68-L73 for the TaskType Class\n",
    "    target_modules = CFG.LORA_MODULES # Only use Output and Values Projection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51a27317-6d29-4254-997e-ea3585ba87c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exciting!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# # Create LoRa Model\n",
    "# model = get_peft_model(base_model, lora_config)\n",
    "# # Trainable Parameters\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "903971b0-8da5-4f97-8e0d-795c02932221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # verify teh trainable layers\n",
    "# MODEL_LAYERS_ROWS = []\n",
    "# TRAINABLE_PARAMS = []\n",
    "# N_TRAINABLE_PARAMS = 0\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     # Layer Parameter Count\n",
    "#     n_parameters = int(torch.prod(torch.tensor(param.shape)))\n",
    "#     # Only Trainable Layers\n",
    "#     if param.requires_grad:\n",
    "#         # Add Layer Information\n",
    "#         MODEL_LAYERS_ROWS.append({\n",
    "#             'param': n_parameters,\n",
    "#             'name': name,\n",
    "#             'dtype': param.data.dtype,\n",
    "#         })\n",
    "#         # Append Trainable Parameter\n",
    "#         TRAINABLE_PARAMS.append({'params': param})\n",
    "#         # Add Number of Trainable Parameters\n",
    "#         N_TRAINABLE_PARAMS += n_parameters\n",
    "\n",
    "# display(pd.DataFrame(MODEL_LAYERS_ROWS))\n",
    "\n",
    "# print(f\"\"\"\n",
    "# ===============================\n",
    "# N_TRAINABLE_PARAMS: {N_TRAINABLE_PARAMS:,}\n",
    "# N_TRAINABLE_LAYERS: {len(TRAINABLE_PARAMS)}\n",
    "# ===============================\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e729f0af-df3c-4c7b-8d20-0e97d9b49d4f",
   "metadata": {},
   "source": [
    "### Create tf-idf feature extraction model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "238714c5-c143-40fe-acc3-27c6d49997d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_TFIDF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model_TFIDF,self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, kernel_size=3, stride = 1, bias = True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(8, 16, kernel_size = 3, stride = 1, bias = True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(16 * 896, CFG.NUM_CLASS_LLAMA, bias = True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self,in_feat):\n",
    "        x = self.conv1(in_feat)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(-1,16*896)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "model_tfidf = model_TFIDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aad23496-0202-48d2-a66f-409441e71e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test model\n",
    "# model_tfidf = model_TFIDF()\n",
    "# output = model_tfidf(test_c.view(2,1,900).to(torch.float32).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f9ed448-cc18-4e2a-813d-f0eea0faacf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class compound_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(compound_model,self).__init__()\n",
    "        self.model_tfidf = model_TFIDF().to(DEVICE)\n",
    "        # self.model_tfidf.to(DEVICE)\n",
    "        self.model_lora = get_peft_model(base_model, lora_config)\n",
    "        self.rt = nn.Sequential(\n",
    "            nn.Linear(CFG.NUM_CLASS_LLAMA * 2, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decide = nn.Linear(128, 3)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, _input_token, _input_mask, _input_feat):\n",
    "        output_llama = self.model_lora(_input_token, _input_mask)\n",
    "        output_tfidf = self.model_tfidf(_input_feat)\n",
    "        print(output_llama.shape)\n",
    "        print(output_tfidf.shape)\n",
    "        output_compound = torch.cat((output_llama, output_tfidf),dim=-1)\n",
    "        output = self.decide(output_compound)\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5bbe8c8f-be22-43f6-b2ad-4fef08cd62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compound = compound_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5c57697-963a-4ba3-bc62-ea4c3c185af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc86370-9408-4a82-9169-5e4a13706db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "out = model_compound(test_a.cpu(), test_b.cpu(), test_c.view(2,1,900).to(torch.float32).cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf061895-9aa0-4853-a4b3-01151e0f2d17",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "cd7e7a3b-98b2-4fc3-a8ce-98dc5c04efd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE: 2, N_SAMPLES: 574, STEPS_PER_EPOCH: 287\n"
     ]
    }
   ],
   "source": [
    "# learning rate and optimizer\n",
    "N_SAMPLES = len(train)\n",
    "STEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE\n",
    "\n",
    "OPTIMIZER=torch.optim.AdamW(model.parameters(), lr = CFG.LR_MAX)\n",
    "\n",
    "# Cosine Learning Rate with Warmup\n",
    "lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "    optimizer=OPTIMIZER,\n",
    "    num_warmup_steps=CFG.NUM_WARMUP_STEPS,\n",
    "    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCHS\n",
    ")\n",
    "\n",
    "print(f'BATCH_SIZE: {CFG.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "6cd2f7b6-f6ea-402a-be7b-a7a5199b88de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data type for the optimizer's state(e.g., momentum buffers)\n",
    "for state in OPTIMIZER.state.values():\n",
    "    for k,v in state.items():\n",
    "        if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:\n",
    "            state[v] = v.to(dtype=torch.float32)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "1f77de7d-90bc-4d93-863d-3b1df6f6a5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([2, 128]), dtype: torch.int32\n",
      "attention_mask shape: torch.Size([2, 128]), dtype: torch.int32\n",
      "input_feat shape: torch.Size([2, 900]), dtype: torch.float64\n",
      "labels shape: torch.Size([2, 3]), dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_mask, input_feat, labels = next(TRAIN_DATASET)\n",
    "print(f'input_ids shape: {input_ids.shape}, dtype: {input_ids.dtype}')\n",
    "print(f'attention_mask shape: {attention_mask.shape}, dtype: {attention_mask.dtype}')\n",
    "print(f'input_feat shape: {input_feat.shape}, dtype: {input_feat.dtype}')\n",
    "print(f'labels shape: {labels.shape}, dtype: {labels.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "8cbc1379-8e46-407d-8947-0fc25179619b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=1, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=1, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=1, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=1, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=128, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "37b21d37-1bea-4434-9b21-be5bbfa4f312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jryhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:648: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([[-6.0156e-01,  2.2188e+00, -5.7031e-01, -5.9375e+00,  2.3906e+00,\n",
      "          3.3438e+00,  2.3125e+00,  4.1250e+00, -2.3906e+00,  1.9297e+00,\n",
      "          1.7266e+00,  3.2812e-01,  3.6094e+00,  2.1875e+00,  2.0469e+00,\n",
      "          2.2031e+00, -5.6250e-01,  1.2578e+00, -4.4336e-01,  8.2812e-01,\n",
      "          5.8750e+00,  2.4688e+00, -2.5938e+00,  2.4062e+00, -5.5859e-01,\n",
      "         -3.3438e+00, -1.4062e+00, -2.6250e+00, -3.2188e+00, -3.6406e+00,\n",
      "         -2.3750e+00, -2.2461e-01,  1.2344e+00,  7.6562e+00,  3.7812e+00,\n",
      "          6.7188e-01,  7.5938e+00,  3.7188e+00,  4.7656e-01, -1.8516e+00,\n",
      "          2.7188e+00, -1.5234e+00,  2.9297e-03, -1.1406e+00, -1.5859e+00,\n",
      "          3.4570e-01,  2.5195e-01, -3.2188e+00,  7.0312e-02, -1.2656e+00,\n",
      "         -2.8125e-01, -1.9375e+00, -2.4316e-01,  1.6250e+00,  2.4707e-01,\n",
      "         -7.5000e+00,  2.8125e+00,  2.7812e+00,  2.2812e+00, -1.7422e+00,\n",
      "          4.2500e+00, -4.2188e+00,  1.4375e+00, -5.4062e+00, -7.8438e+00,\n",
      "         -7.3828e-01, -1.3203e+00, -4.0000e+00,  1.2109e+00,  1.3594e+00,\n",
      "         -3.9844e+00, -1.7266e+00, -2.6562e-01,  6.4375e+00, -1.7031e+00,\n",
      "          1.8359e+00,  1.4609e+00,  2.0312e+00, -2.9531e+00,  3.8672e-01,\n",
      "         -4.8438e+00, -2.4688e+00,  3.0078e-01,  6.5625e-01,  2.8516e-01,\n",
      "         -5.8438e+00, -2.4219e+00, -4.4062e+00, -9.2969e-01, -1.0469e+00,\n",
      "         -1.9219e+00,  6.1250e+00, -1.0000e+00, -1.2266e+00, -3.2188e+00,\n",
      "         -5.9766e-01, -1.6016e+00,  1.5703e+00, -7.2656e-01, -5.2344e-01,\n",
      "         -1.5547e+00, -3.8125e+00,  4.4922e-01,  2.0625e+00, -3.9688e+00,\n",
      "         -1.8828e+00, -1.5781e+00,  5.3750e+00,  1.1328e-01, -3.7344e+00,\n",
      "         -4.1875e+00,  8.1250e-01,  3.4375e+00,  2.2031e+00,  1.7344e+00,\n",
      "         -2.9531e+00,  2.2188e+00, -1.0391e+00,  2.9062e+00, -2.8906e+00,\n",
      "         -1.7500e+00,  3.2500e+00, -2.1250e+00, -5.0000e+00,  6.7969e-01,\n",
      "         -2.7344e-02, -2.1875e+00,  4.5117e-01],\n",
      "        [ 2.7656e+00,  1.0469e+00,  2.7188e+00, -2.6875e+00,  3.3438e+00,\n",
      "         -1.0234e+00,  2.9062e+00, -1.2812e+00,  1.3594e+00,  4.7188e+00,\n",
      "         -2.8125e+00, -4.4062e+00, -9.8438e-01,  1.1562e+00,  2.3438e-01,\n",
      "         -4.0938e+00,  3.9844e+00,  5.2188e+00,  5.7500e+00,  3.3398e-01,\n",
      "          7.3750e+00,  1.2109e+00,  1.7344e+00, -1.2891e+00,  1.6602e-01,\n",
      "         -4.5938e+00,  5.0000e+00, -1.9688e+00, -3.9531e+00, -2.2812e+00,\n",
      "          7.6953e-01,  4.2188e+00, -1.3203e+00, -2.5469e+00,  1.8906e+00,\n",
      "          2.6250e+00,  3.6875e+00,  4.8438e+00, -4.0000e+00,  1.7422e+00,\n",
      "         -3.5938e+00,  2.1406e+00, -6.8359e-01,  6.8359e-01, -1.6250e+00,\n",
      "          5.8594e-01,  4.1406e-01, -6.8750e-01, -1.2793e-01,  1.6406e+00,\n",
      "         -8.8672e-01,  1.3125e+00, -3.1094e+00,  5.0000e+00, -5.1562e-01,\n",
      "          1.5625e+00,  7.1484e-01,  3.5312e+00, -2.5312e+00,  1.2422e+00,\n",
      "          9.8828e-01, -6.3281e-01,  2.8320e-02,  2.5781e-01, -7.6562e+00,\n",
      "          2.6406e+00,  1.2188e+00,  2.7969e+00, -5.3750e+00,  5.4688e-01,\n",
      "         -2.2500e+00,  4.5508e-01, -7.6953e-01,  6.3125e+00, -2.7344e+00,\n",
      "         -3.4688e+00,  3.7969e+00, -3.1445e-01, -2.4023e-01,  8.9453e-01,\n",
      "         -1.7109e+00, -5.0781e-02,  1.7188e+00,  5.5000e+00, -1.6172e+00,\n",
      "         -2.1250e+00, -2.5781e+00,  5.1562e+00,  2.7656e+00, -4.9609e-01,\n",
      "         -2.5195e-01,  1.9824e-01, -4.2383e-01, -3.0156e+00, -2.5391e-01,\n",
      "          2.4844e+00, -2.9219e+00,  6.6875e+00, -3.7656e+00,  5.8594e-01,\n",
      "         -2.7344e+00, -1.0391e+00,  3.2500e+00, -1.0781e+00, -3.7969e+00,\n",
      "          1.3203e+00,  1.6250e+00,  7.1875e-01, -1.5469e+00, -1.1328e+00,\n",
      "         -3.0781e+00, -2.1406e+00,  3.4531e+00,  5.4375e+00, -5.4297e-01,\n",
      "          1.7090e-01,  1.4219e+00,  1.2734e+00, -1.8672e+00,  4.0938e+00,\n",
      "          4.0938e+00, -2.8281e+00,  4.4062e+00, -5.0312e+00,  1.2344e+00,\n",
      "         -6.1250e+00, -7.1562e+00, -9.8438e-01]], device='cuda:0',\n",
      "       dtype=torch.bfloat16), dtype: torch.bfloat16\n",
      "CPU times: total: 4.23 s\n",
      "Wall time: 7.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Dummy Prediction\n",
    "print(input_ids.get_device(), attention_mask.get_device())\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "print(f'logits: {outputs.logits}, dtype: {outputs.logits.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "59af8d36-696e-48d1-a9a3-5cd6a8d1be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put model in train_mode\n",
    "model.train()\n",
    "\n",
    "# loss function, cross entropy\n",
    "LOSS_FN = torch.nn.CrossEntropyLoss().to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6abdfe-79d7-4217-88fc-48e293c895f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time()\n",
    "warnings.filterwarnings(\"error\")\n",
    "METRICS={\n",
    "    'loss':[],\n",
    "    'accuracy': {'y_true':[], 'y_pred':[]}\n",
    "}\n",
    "\n",
    "for epoch in tqdm(range(CFG.NUM_EPOCHS)):\n",
    "    ste = time()\n",
    "    for step in range(STEPS_PER_EPOCH):\n",
    "        # Zero out gradients\n",
    "        OPTIMIZER.zero_grad()\n",
    "\n",
    "        # get batch\n",
    "        input_ids, attention_mask, input_feat, labels = next(TRAIN_DATASET)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "\n",
    "        # logits float32\n",
    "        logits = outputs.logits.to(dtype=torch.float32)\n",
    "\n",
    "        # backward pass\n",
    "        loss = LOSS_FN(logits, labels.to(dtype=torch.float32))\n",
    "        loss.backward()\n",
    "\n",
    "        # optimizer step\n",
    "        OPTIMIZER.step()\n",
    "\n",
    "        # update learning rate scheduler\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        METRICS['loss'].append(float(loss))\n",
    "        METRICS['accuracy']['y_true'] += labels.squeeze().tolist()\n",
    "        METRICS['accuracy']['y_pred'] += torch.argmax(F.softmax(logits,dim=-1), dim=1).cpu().tolist()\n",
    "\n",
    "        if (step+1)%200 == 0:\n",
    "            metrics = 'mu_loss: {:.3f}'.format(np.mean(METRICS['loss']))\n",
    "            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])\n",
    "            metrics += ', mu_auc: {:.3f}'.format(accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), axis=-1), \\\n",
    "                        METRICS['accuracy']['y_pred']))\n",
    "            lr = OPTIMIZER.param_groupps[0]['lr']\n",
    "            print(f'{epoch+1:02}/{CFG.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')\n",
    "            print(f'\\nSteps per epoch: {step+1} complete | Time elapsed: {time() - st}')\n",
    "\n",
    "    print(f'\\nEpoch {epoch+1} Completed | Total time for epoch: {time()-ste} ')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
