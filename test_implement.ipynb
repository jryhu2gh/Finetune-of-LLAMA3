{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21977d37-d11a-4e2f-8917-02a984917674",
   "metadata": {},
   "source": [
    "## Import related packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89979461-9f26-4303-8994-d1a31aeda035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc # garbage collection. help with memory management\n",
    "from time import time\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# finetuning related modules\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "# end\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# TPU related, not used\n",
    "\"\"\"\n",
    "import torch_xla.debug.profiler as xp\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.experimental.xla_sharding as xs\n",
    "import torch_xla.runtime as xr\n",
    "\n",
    "xr.use_spmd()\n",
    "\n",
    "from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\n",
    "from torch_xla.experimental.xla_sharding import Mesh\n",
    "from spmd_util import partition_module\n",
    "\"\"\"\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "print(f'Torch Version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b70519f-695a-4de1-80d1-7a0297ff47aa",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46ce1e33-2219-436f-adc9-869280cc1b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is used\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    NUM_EPOCHS = 1\n",
    "    BATCH_SIZE = 2\n",
    "    DROPOUT = 0.05\n",
    "    MODEL_NAME = \"../llama3/Meta-Llama-3-8B/\"\n",
    "    SEED = 2024\n",
    "    MAX_LENGTH = 512\n",
    "    NUM_WARMUP_STEPS = 128\n",
    "    LR_MAX = 5E-5\n",
    "    NUM_LABELS = 3\n",
    "    LORA_RANK = 2\n",
    "    LORA_ALPHA = 8\n",
    "    LORA_MODULES = ['o_proj', 'v_proj']\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    print('GPU is used')\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    print('CPU is used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "370fdef6-5825-4750-8cf6-2f751139a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc4e6e11-7803-4969-b771-0a0bf18dd3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to ensure reproducibility\n",
    "def set_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seeds(seed=CFG.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9065486-c58a-450b-800b-bf0f0a616bfb",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "069ff862-063c-4267-b59a-fb8019de1247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tokenizer\\\\tokenizer_config.json',\n",
       " 'tokenizer\\\\special_tokens_map.json',\n",
       " 'tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_size = 'right'\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.tokenize(\"shuo bu shuo!\")\n",
    "\n",
    "tokenizer.save_pretrained('tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80232bfa-d306-46e2-bf84-efb36db78b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function giving token length\n",
    "# only takes data frame input?\n",
    "def get_token_lengths(texts):\n",
    "    # tokenize and receive inputs_ids for each text\n",
    "    inputs_ids = tokenizer(texts.tolist(),return_tensors='np')['input_ids']\n",
    "    # input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']\n",
    "    # return length of inputs_ids for each text\n",
    "    return [len(t) for t in inputs_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1556c7d4-9d7b-4325-af52-f2380606f38a",
   "metadata": {},
   "source": [
    "# Prepare train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a55adec0-3c3d-4504-8940-22a2f48e43a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../lmsys-chatbot-arena/train.csv')\n",
    "def process(input_str):\n",
    "    # remove the [ and ] and the begin and end of text\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    # split the string with \",\", then remove \"\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    # combine the strings with space\n",
    "    return ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00678a8a-42d3-420c-848d-4fc4d5c418e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"What is the difference between marriage license and marriage certificate?\",\"How can I get both of them as quick as possible in California \",\"What is the minimal time to get them? 1 day or 10 days?\"]\n",
      "--------------------------------------------------\n",
      "What is the difference between marriage license and marriage certificate? How can I get both of them as quick as possible in California  What is the minimal time to get them? 1 day or 10 days?\n"
     ]
    }
   ],
   "source": [
    "## Check\n",
    "before_process = train_df.loc[1,'prompt']\n",
    "after_process = train_df['prompt'].apply(process)\n",
    "# train.loc[:,'prompt'] = train['prompt'].apply(process)\n",
    "print(before_process)\n",
    "print('-'*50)\n",
    "print(after_process.loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2df06c83-640d-4eff-8f9c-4273f060fd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 19 Null reponse rows dropped\n",
      "Total train samples:  57458\n"
     ]
    }
   ],
   "source": [
    "# do the process for all data\n",
    "train_df.loc[:, 'prompt'] = train_df['prompt'].apply(process)\n",
    "train_df.loc[:, 'response_a'] = train_df['response_a'].apply(process)\n",
    "train_df.loc[:, 'response_b'] = train_df['response_b'].apply(process)\n",
    "\n",
    "# Drop 'Null' for training\n",
    "condition = (train_df.response_a=='null') & (train_df.response_b=='null')\n",
    "indexes = train_df[condition].index\n",
    "# print(indexes)\n",
    "train_df.drop(indexes, inplace=True)\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(f\"Total {len(indexes)} Null reponse rows dropped\")\n",
    "print('Total train samples: ', len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2aaa807-e32f-444e-b783-56d84889ebf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>Is it morally right to try to have a certain p...</td>\n",
       "      <td>The question of whether it is morally right to...</td>\n",
       "      <td>As an AI, I don't have personal beliefs or opi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>What is the difference between marriage licens...</td>\n",
       "      <td>A marriage license is a legal document that al...</td>\n",
       "      <td>A marriage license and a marriage certificate ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>explain function calling. how would you call a...</td>\n",
       "      <td>Function calling is the process of invoking or...</td>\n",
       "      <td>Function calling is the process of invoking a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96401</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>How can I create a test set for a very rare ca...</td>\n",
       "      <td>Creating a test set for a very rare category c...</td>\n",
       "      <td>When building a classifier for a very rare cat...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198779</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>What is the best way to travel from Tel-Aviv t...</td>\n",
       "      <td>The best way to travel from Tel Aviv to Jerusa...</td>\n",
       "      <td>The best way to travel from Tel-Aviv to Jerusa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             model_a              model_b  \\\n",
       "0   30192  gpt-4-1106-preview           gpt-4-0613   \n",
       "1   53567           koala-13b           gpt-4-0613   \n",
       "2   65089  gpt-3.5-turbo-0613       mistral-medium   \n",
       "3   96401    llama-2-13b-chat  mistral-7b-instruct   \n",
       "4  198779           koala-13b   gpt-3.5-turbo-0314   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Is it morally right to try to have a certain p...   \n",
       "1  What is the difference between marriage licens...   \n",
       "2  explain function calling. how would you call a...   \n",
       "3  How can I create a test set for a very rare ca...   \n",
       "4  What is the best way to travel from Tel-Aviv t...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  The question of whether it is morally right to...   \n",
       "1  A marriage license is a legal document that al...   \n",
       "2  Function calling is the process of invoking or...   \n",
       "3  Creating a test set for a very rare category c...   \n",
       "4  The best way to travel from Tel Aviv to Jerusa...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  As an AI, I don't have personal beliefs or opi...               1   \n",
       "1  A marriage license and a marriage certificate ...               0   \n",
       "2  Function calling is the process of invoking a ...               0   \n",
       "3  When building a classifier for a very rare cat...               1   \n",
       "4  The best way to travel from Tel-Aviv to Jerusa...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               1           0  \n",
       "2               0           1  \n",
       "3               0           0  \n",
       "4               1           0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "905c41ff-2905-444e-9bea-25aa2ec41a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User prompt: What is the best way to travel from Tel-Aviv to Jerusalem? Car? Bus? Plane?\n",
      "\n",
      "Model A:\n",
      "The best way to travel from Tel Aviv to Jerusalem depends on your personal preference and the availability of transportation options. All the options you have mentioned are valid options, but here are some details to help you make your decision:\\n\\n*   By car: Traveling by car is the quickest way to get from Tel Aviv to Jerusalem, as the distance between the two cities is only about 60 kilometers (37 miles). It takes around 45 minutes to drive from Tel Aviv to Jerusalem by car, depending on the traffic.\\n*   By bus: There are several bus lines that run from Tel Aviv to Jerusalem, and the journey takes around 1 hour and 30 minutes by bus. The buses are comfortable and reliable, and they offer a scenic view of the beautiful Israeli countryside.\\n*   By plane: There are no direct flights from Tel Aviv to Jerusalem, so you need to take a flight from Tel Aviv's Ben Gurion International Airport to Jerusalem's Sde Dov Airport. The flight takes around 30 minutes, and it is a convenient option if you are short on time.\\n\\nIn general, if you have enough time and don't mind taking the bus, I recommend that option. The buses are comfortable, reliable, and offer a scenic view of the countryside. If you are short on time and don't want to take a bus, a flight is another option to consider.\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "The best way to travel from Tel-Aviv to Jerusalem depends on your personal preferences and circumstances. \\n\\nIf you have a car and enjoy driving, then driving from Tel-Aviv to Jerusalem is an option. However, parking in Jerusalem can be challenging and expensive.\\n\\nIf you prefer to use public transportation, there are several bus lines that operate between Tel-Aviv and Jerusalem. Some of the most popular bus companies include Egged and Dan. The bus ride typically takes about an hour, depending on traffic.\\n\\nTaking a plane is not a recommended option since Tel-Aviv and Jerusalem are relatively close cities, and there are no airports in Jerusalem. \\n\\nIn summary, taking a bus is the most commonly used and convenient way to travel from Tel-Aviv to Jerusalem.\n"
     ]
    }
   ],
   "source": [
    "# Show the conversation\n",
    "train_df['text']='User prompt: '+train_df['prompt']+ '\\n\\nModel A:\\n'+train_df['response_a']+'\\n\\n--------\\n\\nModel B:\\n'+train_df['response_b']\n",
    "print(train_df['text'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1053b805-0b5d-4750-a7b0-897438106f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c12b38a-d693-4dcd-ac94-3e378c60b62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jryhu\\AppData\\Local\\Temp\\ipykernel_36668\\1975162896.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train.loc[:,'token_count'] = get_token_lengths(texts)\n",
      "C:\\Users\\jryhu\\AppData\\Local\\Temp\\ipykernel_36668\\1975162896.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train.loc[:, 'label']=np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "      <th>text</th>\n",
       "      <th>token_count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>Is it morally right to try to have a certain p...</td>\n",
       "      <td>The question of whether it is morally right to...</td>\n",
       "      <td>As an AI, I don't have personal beliefs or opi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>User prompt: Is it morally right to try to hav...</td>\n",
       "      <td>1206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>What is the difference between marriage licens...</td>\n",
       "      <td>A marriage license is a legal document that al...</td>\n",
       "      <td>A marriage license and a marriage certificate ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>User prompt: What is the difference between ma...</td>\n",
       "      <td>1393</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>explain function calling. how would you call a...</td>\n",
       "      <td>Function calling is the process of invoking or...</td>\n",
       "      <td>Function calling is the process of invoking a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>User prompt: explain function calling. how wou...</td>\n",
       "      <td>664</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96401</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>How can I create a test set for a very rare ca...</td>\n",
       "      <td>Creating a test set for a very rare category c...</td>\n",
       "      <td>When building a classifier for a very rare cat...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>User prompt: How can I create a test set for a...</td>\n",
       "      <td>1008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198779</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>What is the best way to travel from Tel-Aviv t...</td>\n",
       "      <td>The best way to travel from Tel Aviv to Jerusa...</td>\n",
       "      <td>The best way to travel from Tel-Aviv to Jerusa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>User prompt: What is the best way to travel fr...</td>\n",
       "      <td>479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             model_a              model_b  \\\n",
       "0   30192  gpt-4-1106-preview           gpt-4-0613   \n",
       "1   53567           koala-13b           gpt-4-0613   \n",
       "2   65089  gpt-3.5-turbo-0613       mistral-medium   \n",
       "3   96401    llama-2-13b-chat  mistral-7b-instruct   \n",
       "4  198779           koala-13b   gpt-3.5-turbo-0314   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Is it morally right to try to have a certain p...   \n",
       "1  What is the difference between marriage licens...   \n",
       "2  explain function calling. how would you call a...   \n",
       "3  How can I create a test set for a very rare ca...   \n",
       "4  What is the best way to travel from Tel-Aviv t...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  The question of whether it is morally right to...   \n",
       "1  A marriage license is a legal document that al...   \n",
       "2  Function calling is the process of invoking or...   \n",
       "3  Creating a test set for a very rare category c...   \n",
       "4  The best way to travel from Tel Aviv to Jerusa...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  As an AI, I don't have personal beliefs or opi...               1   \n",
       "1  A marriage license and a marriage certificate ...               0   \n",
       "2  Function calling is the process of invoking a ...               0   \n",
       "3  When building a classifier for a very rare cat...               1   \n",
       "4  The best way to travel from Tel-Aviv to Jerusa...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \\\n",
       "0               0           0   \n",
       "1               1           0   \n",
       "2               0           1   \n",
       "3               0           0   \n",
       "4               1           0   \n",
       "\n",
       "                                                text  token_count  label  \n",
       "0  User prompt: Is it morally right to try to hav...         1206      0  \n",
       "1  User prompt: What is the difference between ma...         1393      1  \n",
       "2  User prompt: explain function calling. how wou...          664      2  \n",
       "3  User prompt: How can I create a test set for a...         1008      0  \n",
       "4  User prompt: What is the best way to travel fr...          479      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train only 50% train dataset\n",
    "train = train_df[:int(len(train_df)/2)]\n",
    "texts = train['text']\n",
    "train.loc[:,'token_count'] = get_token_lengths(texts)\n",
    "\n",
    "# prepare label for model\n",
    "train.loc[:, 'label']=np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bea7e0fa-f47e-4346-ad07-fefc15240b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    10092\n",
       "1     9837\n",
       "2     8800\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ce41bcc-8ebe-4f0a-81af-4641a41b79be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28729.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>729.613526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>768.325978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>288.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>563.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15428.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        token_count\n",
       "count  28729.000000\n",
       "mean     729.613526\n",
       "std      768.325978\n",
       "min       18.000000\n",
       "25%      288.000000\n",
       "50%      563.000000\n",
       "75%      900.000000\n",
       "max    15428.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# token Count\n",
    "display(train['token_count'].describe().to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "912ce9ac-fe69-46d8-8205-016c5a083bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1399.2000000000007"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get length of tokens which covers 90% of data, we'll still take 1024 length\n",
    "np.percentile(train['token_count'],90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f0988a-3f01-494b-b104-41d2b46abe8e",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84b1cc80-19ef-43f7-8eb9-22ac52e50dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 40], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.MAX_LENGTH\n",
    "tokenizer(\"I have an apple\",padding='max_length',max_length=2,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76a326db-67fc-415f-af32-7cf52c9682d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_IDS shape: (28729, 512), ATTENTION_MASKS shape: (28729, 512)\n",
      "LABELS shape: (28729, 3)\n"
     ]
    }
   ],
   "source": [
    "# tokenize data\n",
    "tokens = tokenizer(\n",
    "    train['text'].tolist(),\n",
    "    padding='max_length',\n",
    "    max_length=CFG.MAX_LENGTH,\n",
    "    truncation=True,\n",
    "    return_tensors='np'\n",
    ")\n",
    "\n",
    "# Input IDs are teh token IDs\n",
    "INPUT_IDS = tokens['input_ids']\n",
    "# Attention Masks to Ignore Padding Tokens\n",
    "ATTENTION_MASKS = tokens['attention_mask']\n",
    "# Label of Texts\n",
    "LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values\n",
    "\n",
    "print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')\n",
    "print(f'LABELS shape: {LABELS.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61d1d837-d6a3-4fc2-8dde-c8c353b40c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataset(batch_size):\n",
    "    N_SAMPLES = LABELS.shape[0]\n",
    "    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))\n",
    "    while True:\n",
    "        # Shuffle Indices\n",
    "        np.random.shuffle(IDXS)\n",
    "        # Iterate Over All Indices Once\n",
    "        for idxs in IDXS.reshape(-1, batch_size):\n",
    "            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)\n",
    "            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)\n",
    "            labels = torch.tensor(LABELS[idxs]).to(DEVICE) # Multi-label\n",
    "\n",
    "            # yield returns a returns a generator object to \n",
    "            # the one who calls the function which contains \n",
    "            # yield, instead of simply returning a value\n",
    "            yield input_ids, attention_mask, labels\n",
    "            \n",
    "TRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a651715b-0c01-4a3e-ac1f-979253f751c2",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2981a3bc-5a46-4532-89c6-ca3c6e3d2dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d1a50b871d4396bf114009e1be8f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ../llama3/Meta-Llama-3-8B/ and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model for classification with 3 target label\n",
    "base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "    CFG.MODEL_NAME,\n",
    "    num_labels=CFG.NUM_LABELS,\n",
    "    torch_dtype = torch.bfloat16)\n",
    "\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "# Assign Padding TOKEN\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e3a44d5-8919-4bfa-bba3-82f3d1a13b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128001\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d3d87-b227-4356-9359-7a6fbc855bd2",
   "metadata": {},
   "source": [
    "## Low-Rank Adaption [LORA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5e0e632-b84a-4358-9266-5c171e6476dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.LORA_RANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb990ae4-49b4-4f3b-9468-2f3b8454b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=CFG.LORA_RANK, # the dimension of the low-rank matrices\n",
    "    lora_alpha = CFG.LORA_ALPHA, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "    lora_dropout = CFG.DROPOUT,\n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type = TaskType.SEQ_CLS, # refer to https://github.com/huggingface/peft/blob/v0.8.2/src/peft/utils/peft_types.py#L68-L73 for the TaskType Class\n",
    "    target_modules = CFG.LORA_MODULES # Only use Output and Values Projection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe021f68-4291-4b3e-8c96-947852abfa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 864,256 || all params: 7,505,801,216 || trainable%: 0.0115\n"
     ]
    }
   ],
   "source": [
    "# Exciting!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# Create LoRa Model\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "# Trainable Parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63da3a47-e158-4d17-bade-e0257ff30a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of TPU Nodes ???????????????????????????????????\n",
    "# num_devices = xr.global_runtime_device_count()\n",
    "# mesh_shape = (1, num_devices, 1)\n",
    "# device_ids = np.array(range(num_devices))\n",
    "# mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\n",
    "# # distribute model\n",
    "# partition_module(model, mesh)\n",
    "\n",
    "# print(f'num_devices: {num_devices}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73dfb845-885c-44b1-9af6-f00f2dbe719d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>name</th>\n",
       "      <th>dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8192</td>\n",
       "      <td>base_model.model.model.layers.0.self_attn.v_pr...</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2048</td>\n",
       "      <td>base_model.model.model.layers.0.self_attn.v_pr...</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8192</td>\n",
       "      <td>base_model.model.model.layers.0.self_attn.o_pr...</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8192</td>\n",
       "      <td>base_model.model.model.layers.0.self_attn.o_pr...</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8192</td>\n",
       "      <td>base_model.model.model.layers.1.self_attn.v_pr...</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>8192</td>\n",
       "      <td>base_model.model.model.layers.31.self_attn.v_p...</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2048</td>\n",
       "      <td>base_model.model.model.layers.31.self_attn.v_p...</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>8192</td>\n",
       "      <td>base_model.model.model.layers.31.self_attn.o_p...</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>8192</td>\n",
       "      <td>base_model.model.model.layers.31.self_attn.o_p...</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>12288</td>\n",
       "      <td>base_model.model.score.modules_to_save.default...</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     param                                               name           dtype\n",
       "0     8192  base_model.model.model.layers.0.self_attn.v_pr...  torch.bfloat16\n",
       "1     2048  base_model.model.model.layers.0.self_attn.v_pr...  torch.bfloat16\n",
       "2     8192  base_model.model.model.layers.0.self_attn.o_pr...  torch.bfloat16\n",
       "3     8192  base_model.model.model.layers.0.self_attn.o_pr...  torch.bfloat16\n",
       "4     8192  base_model.model.model.layers.1.self_attn.v_pr...  torch.bfloat16\n",
       "..     ...                                                ...             ...\n",
       "124   8192  base_model.model.model.layers.31.self_attn.v_p...  torch.bfloat16\n",
       "125   2048  base_model.model.model.layers.31.self_attn.v_p...  torch.bfloat16\n",
       "126   8192  base_model.model.model.layers.31.self_attn.o_p...  torch.bfloat16\n",
       "127   8192  base_model.model.model.layers.31.self_attn.o_p...  torch.bfloat16\n",
       "128  12288  base_model.model.score.modules_to_save.default...  torch.bfloat16\n",
       "\n",
       "[129 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================\n",
      "N_TRAINABLE_PARAMS: 864,256\n",
      "N_TRAINABLE_LAYERS: 129\n",
      "===============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verify teh trainable layers\n",
    "MODEL_LAYERS_ROWS = []\n",
    "TRAINABLE_PARAMS = []\n",
    "N_TRAINABLE_PARAMS = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # Layer Parameter Count\n",
    "    n_parameters = int(torch.prod(torch.tensor(param.shape)))\n",
    "    # Only Trainable Layers\n",
    "    if param.requires_grad:\n",
    "        # Add Layer Information\n",
    "        MODEL_LAYERS_ROWS.append({\n",
    "            'param': n_parameters,\n",
    "            'name': name,\n",
    "            'dtype': param.data.dtype,\n",
    "        })\n",
    "        # Append Trainable Parameter\n",
    "        TRAINABLE_PARAMS.append({'params': param})\n",
    "        # Add Number of Trainable Parameters\n",
    "        N_TRAINABLE_PARAMS += n_parameters\n",
    "\n",
    "display(pd.DataFrame(MODEL_LAYERS_ROWS))\n",
    "\n",
    "print(f\"\"\"\n",
    "===============================\n",
    "N_TRAINABLE_PARAMS: {N_TRAINABLE_PARAMS:,}\n",
    "N_TRAINABLE_LAYERS: {len(TRAINABLE_PARAMS)}\n",
    "===============================\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ecf0e9-d022-4f41-9fd0-1dafe396ea11",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b786439a-abe7-46d9-bc79-0c7c659da5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE: 2, N_SAMPLES: 28729, STEPS_PER_EPOCH: 14364\n"
     ]
    }
   ],
   "source": [
    "# learning rate and optimizer?\n",
    "N_SAMPLES = len(train)\n",
    "STEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE\n",
    "\n",
    "OPTIMIZER=torch.optim.AdamW(model.parameters(), lr = CFG.LR_MAX)\n",
    "\n",
    "# Cosine Learning Rate with Warmup\n",
    "lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "    optimizer=OPTIMIZER,\n",
    "    num_warmup_steps=CFG.NUM_WARMUP_STEPS,\n",
    "    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCHS\n",
    ")\n",
    "\n",
    "print(f'BATCH_SIZE: {CFG.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3d40e10-5753-426a-9a6d-10be6af97e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data type for the optimizer's state(e.g., momentum buffers)\n",
    "for state in OPTIMIZER.state.values():\n",
    "    for k,v in state.items():\n",
    "        if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:\n",
    "            state[v] = v.to(dtype=torch.float32)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7dce640a-0895-4f68-b66b-3000bf041fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([2, 512]), dtype: torch.int32\n",
      "attention_mask shape: torch.Size([2, 512]), dtype: torch.int32\n",
      "labels shape: torch.Size([2, 3]), dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_mask, labels = next(TRAIN_DATASET)\n",
    "print(f'input_ids shape: {input_ids.shape}, dtype: {input_ids.dtype}')\n",
    "print(f'attention_mask shape: {attention_mask.shape}, dtype: {attention_mask.dtype}')\n",
    "print(f'labels shape: {labels.shape}, dtype: {labels.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bfd1916e-0084-407d-8b57-a98bae8ac22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=2, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=2, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=2, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=2, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=3, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=3, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b49c1b6a-19ab-4259-ac1a-94c2e094ec6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jryhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:648: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([[-0.1641,  2.9062,  4.1562],\n",
      "        [-0.7070,  4.3125, -0.4434]], device='cuda:0', dtype=torch.bfloat16), dtype: torch.bfloat16\n",
      "CPU times: total: 9.55 s\n",
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Dummy Prediction\n",
    "print(input_ids.get_device(), attention_mask.get_device())\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "print(f'logits: {outputs.logits}, dtype: {outputs.logits.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb689d9a-6968-47c5-809c-ad12835bd4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put model in train_mode\n",
    "model.train()\n",
    "\n",
    "# loss function, cross entropy\n",
    "LOSS_FN = torch.nn.CrossEntropyLoss().to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4341d4d-1bc3-4c88-bc58-7890e4f25307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14364"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.NUM_EPOCHS\n",
    "STEPS_PER_EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2758f2cf-3b4a-45bd-bd6a-364957027875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f21fe251c540a5ba59714168b50cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "st = time()\n",
    "warnings.filterwarnings(\"error\")\n",
    "METRICS={\n",
    "    'loss':[],\n",
    "    'accuracy': {'y_true':[], 'y_pred':[]}\n",
    "}\n",
    "\n",
    "for epoch in tqdm(range(CFG.NUM_EPOCHS)):\n",
    "    ste = time()\n",
    "    for step in range(STEPS_PER_EPOCH):\n",
    "        # Zero out gradients\n",
    "        OPTIMIZER.zero_grad()\n",
    "\n",
    "        # get batch\n",
    "        input_ids, attention_mask, labels = next(TRAIN_DATASET)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "\n",
    "        # logits float32\n",
    "        logits = outputs.logits.to(dtype=torch.float32)\n",
    "\n",
    "        # backward pass\n",
    "        loss = LOSS_FN(logits, labels.to(dtype=torch.float32))\n",
    "        loss.backward()\n",
    "\n",
    "        # optimizer step\n",
    "        OPTIMIZER.step()\n",
    "\n",
    "        # update learning rate scheduler\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        METRICS['loss'].append(float(loss))\n",
    "        METRICS['accuracy']['y_true'] += labels.squeeze().tolist()\n",
    "        METRICS['accuracy']['y_pred'] += torch.argmax(F.softmax(logits,dim=-1), dim=1).cpu().tolist()\n",
    "\n",
    "        if (step+1)%200 == 0:\n",
    "            metrics = 'mu_loss: {:.3f}'.format(np.mean(METRICS['loss']))\n",
    "            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])\n",
    "            metrics += ', mu_auc: {:,3f}'.format(accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), axis=-1), \\\n",
    "                        METRICS['accuracy']['y_pred']))\n",
    "            lr = OPTIMIZER.param_groupps[0]['lr']\n",
    "            print(f'{epoch+1:02}/{CFG.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')\n",
    "            print(f'\\nSteps per epoch: {step+1} complete | Time elapsed: {time() - st}')\n",
    "\n",
    "    print(f'\\nEpoch {epoch+1} Completed | Total time for epoch: {time()-ste} ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd749388-0523-450a-8550-2b6c4d909dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
